\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{makecell}
\usepackage{graphicx}
\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}

\title{%
  LLM‑compressor. Отчёт\\[0.5ex]
  {\large\itshape Итоговый проект курса Deep Learning School}%
}
\author{%
  Студент: Матюнина Юлия Алексеевна\\
  Руководитель: Алексей Дмитриевич Рухович%
}
\date{}

\begin{document}
\maketitle

\section*{Введение}

Данная работа сделана к качестве итогового проекта курса от Deep Learning School.

Необходимо было на базе LLM с помощью алгоритма арифметической компрессии реализовать архиватор текста. Сжать Википедию и посчитать её объём. Улучшить коэффициент компрессии с помощью prefix tuning или steering. Обучить модель генерировать распределение для steering-вектора, чтобы ещё сильнее улучшить компрессию.

Основная часть работы находится в ноутбуках \texttt{Compression with different models.ipynb}, \texttt{Compression with steering vector.ipynb} и \texttt{Steering vector with hyper prior.ipynb}.

\section*{Подготовка}

При подготовке работы были изучены научные статьи
\begin{itemize}
  \item \href{https://arxiv.org/pdf/2309.10668}{Language Modeling is Compression},
  \item \href{https://arxiv.org/pdf/1802.01436}{Variational image compression with a scale hyperprior}.
\end{itemize}
Для сжатия используется датасет Wikipedia
\href{https://www.kaggle.com/datasets/nightfury1103/enwik8}{Enwik8}.

Запуск моделей производился на Colab с использованием графического процессора T4.

\section*{Без чанков или с чанками}

В начале обоснуем необходимость разбиения на чанки при экспериментах. В этом разделе использовалась модель \texttt{EleutherAI/pythia-70m}. Но результаты будут схожими и для других моделей.

Всего сжималось 400\,000 бит информации (50\,кБ). Весь объём разбивался на части по 16\,000 бит (2\,кБ). Получается 25 чанков.
Ниже приведена сравнительная таблица результатов сжатия без использования чанков и с разбиением на чанки:

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lrrrr}
    \toprule
    Метод        & Время кодирования, с & Время декодирования, с & Размер после сжатия, бит & Коэффициент сжатия \\
    \midrule
    Без чанков   & 733.66               & 731.65                 & 60\,471                   & 0.1512             \\
    С чанками    & 138.17               & 152.33                 & 66\,303                   & 0.1658             \\
    \bottomrule
  \end{tabular}%
}
\caption{Сравнение без чанков и с чанками}
\end{table}

\begin{itemize}
  \item Разбиение на чанки даёт почти в $5\times$ ускорение кодирования и декодирования.
  \item При этом коэффициент сжатия слегка ухудшается (с 0.1512 до 0.1658), то есть итоговый объём возрастает на $\sim10\%$.
  \item Так как мы располагаем сравнительно небольшими мощностями, для нас ключевым становится фактор времени. Поэтому все дальнейшие эксперименты мы проводим, используя разбиение на чанки.
\end{itemize}

\section*{Различные модели}

Всего сжималось 400\,000 бит информации (50\,кБ) тем же образом — 25 чанков по 2\,кБ.
Код экспериментов находится в ноутбуке \texttt{Compression with different models.ipynb}.
Ниже приведена сравнительная таблица результатов сжатия с использованием различных моделей:

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lcrrrr}
    \toprule
    Модель                  & Параметров & Кодирование (с) & Декодирование (с) & Размер после сжатия (бит) & Коэффициент сжатия \\
    \midrule
    EleutherAI/pythia-70m   & 70\,M      & 138.17          & 152.33            & 66\,303                    & 0.1658             \\
    EleutherAI/pythia-160m  & 160\,M     & 335.48          & 339.62            & 57\,478                    & 0.1437             \\
    GPT-2 (small)           & 117\,M     & 379.86          & 380.13            & 61\,388                    & 0.1535             \\
    Open\_llama\_3b         & 3\,B       & 3297.45         & 3263.61           & 38\,099                    & 0.0952             \\
    Open\_llama\_7b         & 7\,B       & 3971.34         & 3957.65           & 36\,262                    & 0.0907             \\
    \bottomrule
  \end{tabular}%
}
\caption{Результаты по моделям}
\end{table}

Мы видим, что самые маленькие модели (Pythia-70M, Pythia-160M, GPT-2) производят кодирование за несколько сотен секунд, тогда как крупные OpenLLaMA-3B/7B требуют уже нескольких тысяч секунд на энкодинг/декодинг. С ростом размера модели коэффициент сжатия падает (то есть сжатие становится более эффективным). Интересно, что GPT-2, которая имеет меньше параметров, чем EleutherAI/pythia-160m, работает дольше. Тем не менее коэффициент сжатия у неё хуже. Модели Open\_llama\_3b и Open\_llama\_7b отличаются по качеству и скорости сжатия не так сильно, как можно было бы подумать, глядя на количество параметров.

Так как мы располагаем малым количеством вычислительных ресурсов, для дальнейших экспериментов будем использовать EleutherAI/pythia-70m — самую маленькую и быструю модель.

\section*{Steering vector}

\textbf{Steering vector} — это дополнительный контекст, который подаётся в начало входа модели и «настраивает» её работу под нужную тематику или стиль. В наших экспериментах мы рассматривали три варианта steering vector:

\subsection*{Фиксированная строка}
Подаём заранее заданную фразу (в нашем случае, «This is Wikipedia html») и преобразуем её в токены. Эта последовательность служит префиксом, но сама не меняется в ходе работы модели.

\subsection*{Статический soft‑prompt}
Вместо текстовой строки вектор представляет собой матрицу вещественных эмбеддингов фиксированного размера. Элементы этой матрицы инициализируются случайно и после этого не обучаются.

\subsection*{Обучаемый soft‑prompt}
Матрица эмбеддингов включается в процесс обучения. Её значения оптимизируются так, чтобы улучшить степень компрессии заданного текста.

\begin{table}[ht]
\centering
\begin{tabular}{lrr}
\toprule
Вариант steering‑вектора         & Размер после сжатия (бит) & Коэффициент сжатия \\
\midrule
Фиксированная строка             & 66\,406                    & 0.1660             \\
Статический soft‑prompt          & 68\,138                    & 0.1703             \\
Обучаемый soft‑prompt (100 эпох) & 62\,589                    & 0.1565             \\
Обучаемый soft‑prompt (500 эпох) & 54\,256                    & 0.1356             \\
\bottomrule
\end{tabular}
\caption{Влияние вида steering vector}
\end{table}

Добавление фиксированной текстовой строки или статического soft‑prompt чуть ухудшает коэффициент по сравнению с базовым (0.1658). Это логично, ведь мы подаем рандомные числа, которые никак не отражают информацию в тексте.
Оптимизация soft‑prompt в ходе обучения (100 и особенно 500 эпох) заметно улучшает сжатие, снижая коэффициент до 0.1356.

\section*{Hyper‑prior}

Гипер‑приор (hyper‑prior) — это распределение априорной информации $c$ (в нашем случае это steering vector), которое позволяет учесть априорные представления о данных $x$. Формально мы представляем плотность $p(x)$ как:
\[
  p(x) \;=\;\int p(c)\,p(x\mid c)\,\mathrm{d}c.
\]
Кодирование происходит в два этапа:
\begin{enumerate}
  \item Сначала кодируется значение $c$ по априорному распределению $p(c)$.
  \item Затем кодируется текст $x$ по условному распределению $p(x\mid c)$.
\end{enumerate}
Такой подход позволяет добиться более эффективного сжатия, так как мы сжимаем не только $x$, но и $c$.

Мы протестировали три варианта кодирования steering vector по hyper‑prior:

\subsection*{Вариант 1: Гауссов гипер‑приор}

Предполагаем, что каждая координата $c_i$ steering‑вектора распределена нормально: $c_i \sim \mathcal{N}(0,\sigma^2)$, и разбиваем диапазон $[c_{\min},c_{\max}]$ на $N$ равномерных бинов длины $\Delta=(c_{\max}-c_{\min})/N$. Априорная дискретная вероятность каждого бина строится по Гауссову закону, после чего из PMF строится CDF для арифметического кодирования.

\subsection*{Вариант 2: Эмпирический гипер‑приор}

Используем фактические значения $\{c_i\}$ из обученного steering‑вектора:
\begin{enumerate}
  \item Отбрасываем экстремальные перцентили (1\% и 99\%), получая диапазон $[v_{\min},v_{\max}]$.
  \item Строим гистограмму на этом отрезке с $N$ бинами, нормируем в PMF.
  \item Строим целочисленный CDF из этой PMF.
\end{enumerate}

\subsection*{Вариант 3: Lloyd–Max (1D k‑means) гипер‑приор}

Оптимизируем уровни квантования так, чтобы минимизировать MSE:
\begin{enumerate}
  \item Берём подвыборку значений $c_i$ и запускаем 1D k‑means с $N$ кластерами, получая центроиды $\mu_0,\dots,\mu_{N-1}$.
  \item Каждый $c_i$ «попадает» в ближайший центроид.
  \item Частоты попаданий формируют эмпирический PMF.
  \item Строим целочисленный CDF для арифметического кодирования.
\end{enumerate}

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lrrrrrrrr}
    \toprule
    \makecell[l]{Гипер‑приор} 
      & \makecell[c]{Битов\\использовано}
      & \makecell[c]{Бит/\\коорд}
      & \makecell[c]{Энтр./коорд\\(бит)}
      & \makecell[c]{Общая энтр.\\(бит)}
      & \makecell[c]{MSE}
      & \makecell[c]{Коэф.\\компр. c}
      & \makecell[c]{Текст после\\(бит)}
      & \makecell[c]{Коэф.\\сжатия} \\
    \midrule
    Гауссов                & 690\,257 & 6.7408 & 6.7252 & 688\,656 & 5.10e-4 & 0.2106 & 54\,190 & 0.1355 \\
    Эмпирический           & 795\,086 & 7.7645 & 7.6999 & 788\,475 & 4.30e-3 & 0.2426 & 54\,687 & 0.1367 \\
    Lloyd–Max (1D k‑means) & 788\,823 & 7.7033 & 7.7030 & 788\,789 & 4.14e-5 & 0.2407 & 54\,156 & 0.1354 \\
    \bottomrule
  \end{tabular}%
}
\caption{Сравнение hyper‑prior}
\end{table}

Гауссов prior даёт меньшую энтропию (6.725\,бит/коорд) и требует наименьшее число бит (6.741\,бит/коорд), но MSE у него выше, чем у Lloyd–Max. То есть  При этом коэффициент сжатия steering‑вектора самый низкий (0.2106), и итоговое сжатие текста тоже близко к лучшему (0.1355).
Эмпирический приор имеет наибольшее число бит на координату (7.7645 бит), высокую энтропию и крупная MSE. Соответственно, коэффициент компрессии steering‑вектора худший (0.2426).
Lloyd-Max имеет коэффициент сжатие steering вектора хуже, чем гауссов приор. Однако имеет крайне низкой MSE ($4\cdot 10^{-5}$). Его коэффициент сжатия steering‑вектора (0.2407) приближается к эмпирическому, соответственно итоговый коэффициент сжатия текста — лучший (0.1354).

Таким образом, лучше всего сжимает steering vector гауссов приор. Но восстанавливает после сжатия точнее Lloyd–Max. В зависимости от потребностей надо выбрать один из этих вариантов.

\section*{Выводы}

\begin{itemize}
  \item Мы рассмотрели арифметическое кодирование текста на базе LLM. Нами были рассмотрены модели размером 70m-7b. Чем больше модель, тем лучший коэффициент сжатия она обеспечивает. Но большие модели работают медленнее и требуют больших вычислительных ресурсов.
  \item Предобученный steering vector способен улучшить коэффициент сжатия на несколько процентов. Необученный steering vector использовать нет смысла.
  \item Получившийся steering vector в целях дальнейшей оптимизации можно сам сжать, использую подход на основе hyper prior. Для лучшего коэффициента сжатия steering vector лучше использовать гауссов гипер-приор, а для лучшего коэффициента сжатия самого текста - Lloyd–Max гипер‑приор.
\end{itemize}

\end{document}
