# LLM-compressor

Данная работа сделана к качестве итогового проекта курса от Deep Learning School. 

Необходимо было  на базе LLM с помощью алгоритма арифметической компрессии реализовать архиватор текста. Сжать Википедию и посчитать её объём. Улучшить коэффициент компрессии с помощью prefix tuning или steering. Обучить модель генерировать распределение для steering-вектора, чтобы ещё сильнее улучшить компрессию.

Основная часть работы находится в ноутбуках Train Pipeline.ipynb и Metrics.ipynb.

## Подготовка

При подготовке работы были изучены научные статьи [Language Modeling is Compression](https://arxiv.org/pdf/2309.10668), [Variational image compression with a scale hyperprior].
Для сжатия используется датасет Wikipedia [Enwik8](https://www.kaggle.com/datasets/nightfury1103/enwik8).

Запуск моделей производился на Colab с использованием графического процессора T4.


## Без чанков или с чанками

В начале обоснуем необходимость разбиения на чанки при экспериментах. В этом разделе использовалась модель EleutherAI/pythia-70m. Но результаты будут схожими и для других моделей.

Всего сжималось 400 000 бит информации (50 кБ). Весь объем разбивался на части по 16 000 бит (2 кБ). Получается 25 чанков.
Ниже приведена сравнительная таблица результатов сжатия без использования чанков и с разбиением на чанки:

| Метод      | Время кодирования, с | Время декодирования, с | Размер после сжатия, бит | Коэффициент сжатия |
| ---------- | -------------------- | ---------------------- | ------------------------ | ------------------ |
| Без чанков | 733.66               | 731.65                 | 60 471                   | 0.1512             |
| С чанками  | 137.52               | 139.02                 | 66 303                   | 0.1658             |

* Разбиение на чанки даёт почти в 5 × ускорение кодирования и декодирования.
* При этом коэффициент сжатия слегка ухудшается (с 0.1512 до 0.1658), то есть итоговый объём возрастает на \~10 %.
* Так как мы располагаем сравнительно небольшими мощностями, для нас ключевым становится фактор времени. Поэтому все дальнейшие эксперименты мы проводим, использую развиение на чанки.

## Hyper‑prior

Гипер‑приор (hyper‑prior) — это распределение над «скрытым» кодом $c$, которое позволяет учесть априорные представления о данных $x$. Формально мы представляем плотность $p(x)$ как маргинализацию по этому коду:

$$
p(x) \;=\;\int p(c)\,p(x\mid c)\,\mathrm{d}c.
$$

При энтропийном кодировании делается двухэтапно:

1. Сначала кодируется само значение кода $c$ по априорному распределению $p(c)$.
2. Затем по условному распределению $p(x\mid c)$ кодируется наблюдение $x$.

Такой подход позволяет добиться более эффективного сжатия, потому что мы сначала «закрепляем» ту информацию, которая лежит в $c$, а затем передаём остаток, специфичный для $x$.

### Вариант 1: Гауссов гипер‑приор

Предполагаем, что каждая координата $c_i$ steering‑вектора распределена нормально:

$$
c_i \sim \mathcal{N}(0,\sigma^2),
$$

и разбиваем диапазон $[c_{\min},c_{\max}]$ на $N$ равномерных бинов длины $\Delta=(c_{\max}-c_{\min})/N$. Априорная дискретная вероятность бина с центром $m\Delta + c_{\min} + \tfrac\Delta2$ вычисляется по Гауссу:

$$
p(c_i\text{ в бине }m)
\;\propto\;
\exp\!\Bigl(-\tfrac12\,\bigl(\tfrac{c_{\min}+ (m+\tfrac12)\Delta}{\sigma}\bigr)^2\Bigr).
$$

Из полученного PMF строится целочисленный CDF для Arithmetic Coding.

**Плюсы**:

* Очень просто и быстро.
* Управление «хвостами» через $\sigma$, $c_{\min}$, $c_{\max}$.

**Минусы**:

* Не отражает реального эмпирического распределения значений.
* Может недокодировать «пики» и «провалы» в PMF.

---

### Вариант 2: Эмпирический гипер‑приор

Используем фактические значения $\{c_i\}$ из обученного steering‑вектора:

1. Отбрасываем экстремальные перцентили (например, 1-й и 99-й), получая диапазон $[v_{\min},v_{\max}]$.
2. Строим гистограмму на этом отрезке с $N$ бинами, нормируем в PMF:

   $$
   \hat p(m) = \frac{\#\{\,i: c_i\text{ попал в бин }m\}}{\sum\nolimits_j \#\{c_j\}}.
   $$
3. Строим целочисленный CDF из $\hat p(m)$.

**Плюсы**:

* Точное повторение эмпирики.
* Хорошо ловит асимметрию и мультимодальность.

**Минусы**:

* Требует прохода по данным (O$(\mathrm{len}(c))$).
* Жёстко «подгоняется» под один конкретный вектор.

---

### Вариант 3: Lloyd–Max (1D k‑means) гипер‑приор

Оптимизируем уровни квантования так, чтобы минимизировать MSE квантования:

1. Берём подвыборку значений $\{c_i\}$, запускаем 1D k‑means с $N$ кластерами → центроиды $\{\mu_m\}_{m=0}^{N-1}$.
2. Каждый $c_i$ «попадает» в ближайший центр:

   $$
   m^* = \arg\min_m |c_i - \mu_m|.
   $$
3. Частоты попаданий в кластеры дают эмпирический PMF:

   $$
   \tilde p(m) = \frac{\#\{i: m^* = m\}}{\sum_i 1}.
   $$
4. Строим CDF из $\tilde p(m)$.

**Плюсы**:

* Формально минимизирует MSE при фиксированном числе уровней.
* Часто даёт наилучшее качество приближения.

**Минусы**:

* Дороже (k‑means).
* Centroids могут «прыгать» при новой инициализации.

---

### Итог

Hyper‑prior — мощный инструмент для управления энтропийным кодированием steering‑вектора.

* **Гауссов** вариант прост и универсален.
* **Эмпирический** точно повторяет данные.
* **Lloyd–Max** оптимален по MSE, но чуть сложнее в настройке.
