# LLM-compressor

Данная работа сделана к качестве итогового проекта курса от Deep Learning School. 

Необходимо было  на базе LLM с помощью алгоритма арифметической компрессии реализовать архиватор текста. Сжать Википедию и посчитать её объём. Улучшить коэффициент компрессии с помощью prefix tuning или steering. Обучить модель генерировать распределение для steering-вектора, чтобы ещё сильнее улучшить компрессию.

Основная часть работы находится в ноутбуках Compression with different models.ipynb,  и Steering vector with hyper prior.ipynb.

## Подготовка

При подготовке работы были изучены научные статьи [Language Modeling is Compression](https://arxiv.org/pdf/2309.10668), [Variational image compression with a scale hyperprior].
Для сжатия используется датасет Wikipedia [Enwik8](https://www.kaggle.com/datasets/nightfury1103/enwik8).

Запуск моделей производился на Colab с использованием графического процессора T4.


## Без чанков или с чанками

В начале обоснуем необходимость разбиения на чанки при экспериментах. В этом разделе использовалась модель EleutherAI/pythia-70m. Но результаты будут схожими и для других моделей.

Всего сжималось 400 000 бит информации (50 кБ). Весь объем разбивался на части по 16 000 бит (2 кБ). Получается 25 чанков.
Ниже приведена сравнительная таблица результатов сжатия без использования чанков и с разбиением на чанки:

| Метод      | Время кодирования, с | Время декодирования, с | Размер после сжатия, бит | Коэффициент сжатия |
| ---------- | -------------------- | ---------------------- | ------------------------ | ------------------ |
| Без чанков | 733.66               | 731.65                 | 60 471                   | 0.1512             |
| С чанками  | 138.17               | 152.33                 | 66 303                   | 0.1658             |

* Разбиение на чанки даёт почти в 5 × ускорение кодирования и декодирования.
* При этом коэффициент сжатия слегка ухудшается (с 0.1512 до 0.1658), то есть итоговый объём возрастает на \~10 %.
* Так как мы располагаем сравнительно небольшими мощностями, для нас ключевым становится фактор времени. Поэтому все дальнейшие эксперименты мы проводим, использую развиение на чанки.

## Различные модели

Всего сжималось 400 000 бит информации (50 кБ). Весь объем разбивался на части по 16 000 бит (2 кБ). Получается 25 чанков.
Код экспериментов находится в ноутбуке Compression with different models.ipynb.
Ниже приведена сравнительная таблица результатов сжатия с использованием различных моделей:

Ниже таблица с результатами для разных моделей. Вторым столбцом указан примерный объём (число параметров).

| Модель                 | Параметров | Время кодирования (с) | Время декодирования (с) | Размер после сжатия (бит) | Коэффициент сжатия |
| ---------------------- | :--------: | :-------------------: | :---------------------: | :-----------------------: | :----------------: |
| EleutherAI/pythia-70m  |    70 M    |         138.17        |          152.33         |           66 303          |       0.1658       |
| EleutherAI/pythia-160m |    160 M   |         335.48        |          339.62         |           57 478          |       0.1437       |
| GPT‑2 (small)          |    117 M   |         379.86        |          380.13         |           61 388          |       0.1535       |
| Open\_llama\_3b        |     3 B    |        3297.45        |         3263.61         |           38 099          |       0.0952       |
| Open\_llama\_7b        |     7 B    |        3971.34        |         3957.65         |           36 262          |       0.0907       |

Мы видим, что самые маленькие модели (Pythia‑70M, Pythia‑160M, GPT‑2) производят кодирование за несколько сотен секунд, тогда как крупные OpenLLaMA‑3B/7B требуют уже нескольких тысяч секунд на энкодинг/декодинг.
С ростом размера модели коэффициент сжатия падает (то есть сжатие становится более эффективным). 
Интересно, что GPT‑2, которая имеет меньше праметров, чем EleutherAI/pythia-160m, работает дольше. Тем не менее, коэффициент сжатия у нее хуже.
Модели Open\_llama\_3b и Open\_llama\_7b отличаются по качеству и скорости сжатия не так сильно, как можно было бы подумать, глядя на количество параметров.

Так как мы располагаем малым количеством вычислительных ресурсов, для дальнейших экспериментов будем использовать EleutherAI/pythia-70m - самую маленькую и быструю модель.

## Steering vector

**Steering vector** — это дополнительный контекст, который подаётся в начало входа модели и «настраивает» её работу под нужную тематику или стиль. В наших экспериментах мы рассматрели три варианта steering vector:

1. **Фиксированная строка**
   Подаем заранее заданную фразу (в нашем случае, «This is Wikipedia html») и преобразуем в токены. Эта последовательность служит префиксом, но сама не меняется в ходе работы модели.

2. **Статический soft‑prompt**
   Вместо текстовой строки вектор представляет собой матрицу вещественных эмбеддингов фиксированного размера. Элементы этой матрицы инициализируются случайно и после этого не обучаются.

3. **Обучаемый soft‑prompt**
   Матрица эмбеддингов включается в процесс обучения. Её значения оптимизируются так, чтобы улучшить степень компрессии заданного текста.

## Hyper‑prior

Гипер‑приор (hyper‑prior) — это распределение предварительной информации c (в нашем случае это steering vector), которое позволяет учесть априорные представления о данных x. Формально мы представляем плотность p(x) как:

$$
p(x) = \int p(c) p(x\mid c)\mathrm{d}c.
$$

Кодирование происходит в два этапа:

1. Сначала кодируется значение c по априорному распределению p(c).
2. Затем по условному распределению кодируем сам текст x.

Такой подход позволяет добиться более эффективного сжатия, так как мы сжимаем не только x, но и c.
Мы протестировали три варианта кодирования steering vector по hyper-prior:

### Вариант 1: Гауссов гипер‑приор

Предполагаем, что каждая координата $c_i$ steering‑вектора распределена нормально:

$c_i \sim \mathcal{N}(0,\sigma^2)$,

и разбиваем диапазон $[c_{\min},c_{\max}]$ на $N$ равномерных бинов длины $\Delta=(c_{\max}-c_{\min})/N$. Априорная дискретная вероятность бина с центром $m\Delta + c_{\min} + \tfrac\Delta2$ вычисляется по Гауссу.

Из полученного PMF строится целочисленный CDF для Arithmetic Encoding.

### Вариант 2: Эмпирический гипер‑приор

Используем фактические значения $\{c_i\}$ из обученного steering‑вектора:

1. Отбрасываем экстремальные перцентили (например, 1-й и 99-й), получая диапазон $[v_{\min},v_{\max}]$.
2. Строим гистограмму на этом отрезке с $N$ бинами, нормируем в PMF.
3. Строим целочисленный CDF из $\hat p(m)$.


### Вариант 3: Lloyd–Max (1D k‑means) гипер‑приор

Оптимизируем уровни квантования так, чтобы минимизировать MSE квантования:

1. Берём подвыборку значений \$c\_i\$ и запускаем 1D k‑means с \$N\$ кластерами, получая центроиды \$\mu\_0, \dots, \mu\_{N-1}\$.
2. Каждый элемент \$c\_i\$ «попадает» в ближайший центр.
3. Частоты попаданий в кластеры дают эмпирический PMF.
4. Строим целочисленный CDF для арифметического кодирования.
